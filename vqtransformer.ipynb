{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1567182,"sourceType":"datasetVersion","datasetId":925704},{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179},{"sourceId":12456683,"sourceType":"datasetVersion","datasetId":7857754}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from transformers import AutoTokenizer, TFAutoModel\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random","metadata":{"_uuid":"f7e4ac21-a077-4c6f-b5cf-372c87196521","_cell_guid":"b6f2f5f0-a68b-4790-a48d-28a880347e8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-03T09:07:59.946874Z","iopub.execute_input":"2025-08-03T09:07:59.947152Z","iopub.status.idle":"2025-08-03T09:08:22.175749Z","shell.execute_reply.started":"2025-08-03T09:07:59.947129Z","shell.execute_reply":"2025-08-03T09:08:22.171316Z"}},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1754212087.115028      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver('local')\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    print('die')\n\n'''strategy = tf.distribute.get_strategy()'''","metadata":{"_uuid":"641976da-272a-4902-8282-cc5293b1baa5","_cell_guid":"076c4357-085c-4f6d-ac97-ae9528e221a1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-03T09:08:22.178742Z","iopub.execute_input":"2025-08-03T09:08:22.179156Z","iopub.status.idle":"2025-08-03T09:08:30.336637Z","shell.execute_reply.started":"2025-08-03T09:08:22.179132Z","shell.execute_reply":"2025-08-03T09:08:30.332271Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1754212106.365461      10 service.cc:148] XLA service 0x59e9b1fc2220 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1754212106.365514      10 service.cc:156]   StreamExecutor device (0): TPU, 2a886c8\nI0000 00:00:1754212106.365519      10 service.cc:156]   StreamExecutor device (1): TPU, 2a886c8\nI0000 00:00:1754212106.365522      10 service.cc:156]   StreamExecutor device (2): TPU, 2a886c8\nI0000 00:00:1754212106.365524      10 service.cc:156]   StreamExecutor device (3): TPU, 2a886c8\nI0000 00:00:1754212106.365527      10 service.cc:156]   StreamExecutor device (4): TPU, 2a886c8\nI0000 00:00:1754212106.365530      10 service.cc:156]   StreamExecutor device (5): TPU, 2a886c8\nI0000 00:00:1754212106.365533      10 service.cc:156]   StreamExecutor device (6): TPU, 2a886c8\nI0000 00:00:1754212106.365535      10 service.cc:156]   StreamExecutor device (7): TPU, 2a886c8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'strategy = tf.distribute.get_strategy()'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr30k/captions.txt')","metadata":{"_uuid":"2de2aa7d-45c1-45a6-8c13-9b51a832f82f","_cell_guid":"12af3335-94c1-4552-8507-3ecfebaba504","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-03T09:08:30.338718Z","iopub.execute_input":"2025-08-03T09:08:30.338973Z","iopub.status.idle":"2025-08-03T09:08:30.703414Z","shell.execute_reply.started":"2025-08-03T09:08:30.338941Z","shell.execute_reply":"2025-08-03T09:08:30.698786Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df['image_name']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T09:08:30.705129Z","iopub.execute_input":"2025-08-03T09:08:30.705564Z","iopub.status.idle":"2025-08-03T09:08:30.720515Z","shell.execute_reply.started":"2025-08-03T09:08:30.705539Z","shell.execute_reply":"2025-08-03T09:08:30.715023Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0         1000092795.jpg\n1         1000092795.jpg\n2         1000092795.jpg\n3         1000092795.jpg\n4         1000092795.jpg\n               ...      \n158910     998845445.jpg\n158911     998845445.jpg\n158912     998845445.jpg\n158913     998845445.jpg\n158914     998845445.jpg\nName: image_name, Length: 158915, dtype: object"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"'''df['image_name'] = df['image_name'].apply(lambda x: x.split('#')[0])'''\ndf = df.drop_duplicates(subset='image_name', keep='first')[:256*64]","metadata":{"_uuid":"28b37770-1780-4f69-b055-f5a7619cbdb9","_cell_guid":"65b3b499-e143-42ab-b4d6-aa2ec2fa403c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-03T09:08:30.722296Z","iopub.execute_input":"2025-08-03T09:08:30.722498Z","iopub.status.idle":"2025-08-03T09:08:30.738507Z","shell.execute_reply.started":"2025-08-03T09:08:30.722477Z","shell.execute_reply":"2025-08-03T09:08:30.734924Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"paths = df['image_name'].values\n'''captions = df['caption'].values'''","metadata":{"_uuid":"c5d7e30c-ad64-4bda-870d-f921a81f06ef","_cell_guid":"d3674c1d-7f0c-497f-ad5c-ed3f5491469e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-03T09:08:30.741173Z","iopub.execute_input":"2025-08-03T09:08:30.741384Z","iopub.status.idle":"2025-08-03T09:08:30.753458Z","shell.execute_reply.started":"2025-08-03T09:08:30.741364Z","shell.execute_reply":"2025-08-03T09:08:30.749398Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"captions = df['caption'].values\""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"images = []\nHEIGHT,WIDTH = 128,128\nfor i in paths:\n    img = Image.open('/kaggle/input/flickr30k/flickr30k_images/'+i).convert(\"RGB\")\n    img = img.resize((HEIGHT,WIDTH))\n    img = np.asarray(img)/255.0\n    images.append(img)\n\n\nimages = np.array(images)  # Make sure images is a NumPy array\n\nprint(\"Images shape:\", images.shape)\nprint(\"Images dtype:\", images.dtype)\nprint(\"Images min:\", images.min())  # Use numpy's min method\nprint(\"Images max:\", images.max())  # Use numpy's max method\nprint(\"Images mean:\", images.mean())","metadata":{"_uuid":"14be7143-355d-4899-a0e2-e19cc977555d","_cell_guid":"3dd1d681-245d-4f9f-a9ca-44f44f9fb848","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-03T09:08:30.754422Z","iopub.execute_input":"2025-08-03T09:08:30.755408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vqvae_train = tf.data.Dataset.from_tensor_slices(images)\nvqvae_train = vqvae_train.map(lambda x: tf.cast(x, tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n'''vqvae_train = vqvae_train.shuffle(1000,seed=seed)'''\nvqvae_train = vqvae_train.batch(256, drop_remainder=True)  # Drop the last incomplete batch\nvqvae_train = vqvae_train.prefetch(tf.data.AUTOTUNE).cache()\n\nvqvae_train = strategy.experimental_distribute_dataset(vqvae_train)","metadata":{"_uuid":"150c8d8b-61a8-4096-9e4f-622c841f8cf2","_cell_guid":"5771ec35-a4e3-4d58-bbe8-ed031b176e3f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom IPython.display import clear_output\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\n\n# Hyperparameters\nembedding_dim = 256\nnum_embeddings = 256\ninput_shape = (HEIGHT, WIDTH, 3)\n\n\nroshnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\nroshnet.trainable = False\nname = 'conv3_block4_out'\noutputs = roshnet.get_layer(name).output\nroshnet_model = tf.keras.Model(inputs=roshnet.input, outputs=outputs)\n\n@tf.function\ndef codebook_kl_loss(encodings, num_embeddings, eps=1e-10):\n    # Flatten the encoding indices\n    encodings = tf.reshape(encodings, [-1])\n\n    # One-hot encode the indices\n    one_hot = tf.one_hot(encodings, depth=num_embeddings)\n\n    # Histogram\n    usage = tf.reduce_sum(one_hot, axis=0)\n\n    # Normalize\n    usage = usage / (tf.reduce_sum(usage) + eps)\n\n    # KL divergence to uniform prior (1 / num_embeddings)\n    kl = tf.reduce_sum(usage * tf.math.log((usage + eps) * num_embeddings))\n    return kl\n\nclass VectorQuantizer(layers.Layer):\n    def __init__(self, num_embeddings, embedding_dim,  **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        #self.last_used_indices = None\n        # Create the embeddings (the codebook)\n        self.embeddings = self.add_weight(\n            shape=(self.num_embeddings, self.embedding_dim),\n            initializer=tf.random_uniform_initializer(),\n            trainable=True,\n            name=\"embeddings\",\n        )\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])\n        flat_inputs = tf.cast(flat_inputs, dtype=self.embeddings.dtype)\n    \n        # Compute distances to embedding vectors\n        '''distances = (\n            tf.reduce_sum(flat_inputs ** 2, axis=1, keepdims=True)\n            - 2 * tf.matmul(flat_inputs, self.embeddings, transpose_b=True)\n            + tf.reduce_sum(self.embeddings ** 2, axis=1)\n        )'''\n        flat_inputs_norm = tf.nn.l2_normalize(flat_inputs, axis=1)  # [N, D]\n        embeddings_norm = tf.nn.l2_normalize(self.embeddings, axis=1)  # [K, D]\n        \n        # Cosine similarity = dot product of normalized vectors\n        cosine_sim = tf.matmul(flat_inputs_norm, embeddings_norm, transpose_b=True)  # [N, K]\n        \n        # Convert similarity to distance (cosine distance = 1 - cosine similarity)\n        distances = 1.0 - cosine_sim\n        # Find nearest embedding index\n        encoding_indices = tf.argmin(distances, axis=1)\n    \n        # Get quantized vectors\n        quantized = tf.gather(self.embeddings, encoding_indices)\n        quantized = tf.reshape(quantized, input_shape)\n    \n        # Compute losses\n        e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs) ** 2)\n        q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs)) ** 2)\n    \n        # Entropy loss (you must implement or import this)\n        kldiv_loss = codebook_kl_loss(encoding_indices, num_embeddings=self.num_embeddings)\n    \n        # Total loss\n        loss = q_latent_loss + 0.25 * e_latent_loss + 0.2 * kldiv_loss\n        self.add_loss(loss)\n    \n        # Straight-through estimator\n        quantized = inputs + tf.stop_gradient(quantized - inputs)\n    \n        return quantized  # Optionally, also return encoding_indices\n\ndef residual_block(x, filters):\n    skip = x\n    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.add([x, skip])\n    x = layers.ReLU()(x)\n    return x\n\n# Encoder\nfrom tensorflow.keras.layers import Layer, Conv2D, Add\nimport tensorflow as tf\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Conv2D, Add\nfrom tensorflow.keras.utils import register_keras_serializable\n\n@register_keras_serializable(package=\"Custom\")\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.filters = input_shape[-1]\n        self.f = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.g = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.h = Conv2D(self.filters, kernel_size=1, padding='same')\n        super().build(input_shape)\n\n    def call(self, x):\n        f = self.f(x)  # (B, H, W, C//8)\n        g = self.g(x)\n        h = self.h(x)  # (B, H, W, C)\n\n        shape_f = tf.shape(f)\n        B, H, W = shape_f[0], shape_f[1], shape_f[2]\n\n        f_flat = tf.reshape(f, [B, H * W, self.filters // 8])\n        g_flat = tf.reshape(g, [B, H * W, self.filters // 8])\n        h_flat = tf.reshape(h, [B, H * W, self.filters])\n\n        beta = tf.nn.softmax(tf.matmul(f_flat, g_flat, transpose_b=True), axis=-1)  # (B, N, N)\n\n        o = tf.matmul(beta, h_flat)  # (B, N, C)\n        o = tf.reshape(o, [B, H, W, self.filters])\n\n        return Add()([x, o])  # Residual connection\n\n    def get_config(self):\n        config = super().get_config()\n        # If you have custom arguments in __init__, add them here\n        return config\n\n\ndef get_encoder():\n    inputs = keras.Input(shape=input_shape)\n\n    # Downsample 1: 128x128 → 64x64\n    x = layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(inputs)\n    for _ in range(2): x = residual_block(x, 256)\n\n    # Downsample 2: 64x64 → 32x32\n    x = layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 512)\n\n    # Downsample 3: 32x32 → 16x16\n    x = layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x)\n    for _ in range(3): x = residual_block(x, 512)\n    \n    x = SelfAttention()(x)\n    \n    x = layers.Conv2D(embedding_dim, 1, padding='same')(x)\n    # x = layers.LayerNormalization()(x) \n    # Final latent\n    return keras.Model(inputs, x, name=\"encoder\")\n\n\n# Decoder\ndef get_decoder():\n    inputs = keras.Input(shape=(16, 16, embedding_dim))  # Start from encoded shape\n\n    x = layers.UpSampling2D(size=(2, 2))(inputs)\n    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n    for _ in range(3): x = residual_block(x, 512)\n\n    x = SelfAttention()(x)\n    \n    x = layers.UpSampling2D(size=(2, 2))(x)\n    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 512)\n\n    x = layers.UpSampling2D(size=(2, 2))(x)\n    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 256)\n\n    x = layers.Conv2D(3, 1, padding='same', activation='sigmoid')(x)\n\n    return keras.Model(inputs, x, name=\"decoder\")\n\ndef jom_loss(y_true, y_pred):\n    # Perceptual loss (L1 on VGG features)\n    y_true_resized = tf.image.resize(y_true, (224, 224))*255\n    y_pred_resized = tf.image.resize(y_pred, (224, 224))*255\n\n    y_true_roshnet = tf.keras.applications.resnet50.preprocess_input(y_true_resized)\n    y_pred_roshnet = tf.keras.applications.resnet50.preprocess_input(y_pred_resized)\n    roshnet_true = tf.stop_gradient(roshnet_model(y_true_roshnet))\n    roshnet_pred = roshnet_model(y_pred_roshnet)\n    \n    perceptual_loss = tf.reduce_mean(tf.square(roshnet_true - roshnet_pred))\n    return perceptual_loss\n    \nwith strategy.scope():\n    encoder = get_encoder()\n    decoder = get_decoder()\n    inputs = keras.Input(shape=input_shape)\n    z = encoder(inputs)\n    quantized = VectorQuantizer(num_embeddings, embedding_dim,name='quantizer')(z)\n    reconstructions = decoder(quantized)\n    \n    # This model has more neurons than my brain\n    vqvae = keras.Model(inputs, reconstructions, name=\"vqvae\")\n    \n    vqvae_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\nsample_batch = np.array(images[:5])  # Get 10 sample images for monitoring\n\n# Compile and train\nvqvae.summary()","metadata":{"_uuid":"40d043a1-8d4e-49db-ade5-bc9807b2c896","_cell_guid":"6d2386d1-0b7f-41ea-b517-70636e398d0f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_discriminator(input_shape=(128, 128, 3)):\n    inputs = keras.Input(shape=input_shape)\n\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)  # 128→64\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)       # 64→32\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)       # 32→16\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(512, 4, strides=2, padding='same')(x)       # 16→8\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(1, 4, padding='valid')(x)  # output single scalar or patch (5x5 or so)\n    \n    # Flatten and output\n    x = layers.Flatten()(x)\n    outputs = layers.Activation('sigmoid')(x)  # probability real/fake\n\n    return keras.Model(inputs, outputs, name=\"discriminator\")\n    \nwith strategy.scope():\n    discriminator = get_discriminator()\n    \ndiscriminator.summary()","metadata":{"_uuid":"63d5b3bd-4827-459f-9fa3-db84fd872fb9","_cell_guid":"60963d0c-5a6e-4e16-afe4-9eb2577a3112","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ndef discriminator_loss(real,fake):\n    real_loss = loss(tf.ones_like(real)*0.9,real)\n    fake_loss = loss(tf.zeros_like(fake),fake)\n    total_loss = real_loss+fake_loss\n    return total_loss\n\ndef generator_loss(fake):\n    gen_loss = loss(tf.ones_like(fake),fake)\n    return gen_loss","metadata":{"_uuid":"b0f8d724-9d22-4d93-97b8-6b1f8d43a951","_cell_guid":"cc2cb1bb-f63c-496f-8cbd-3518b8e33ef8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef laplacian_filter():\n    # 3x3 Laplacian kernel\n    kernel = tf.constant([[0.,  1., 0.],\n                          [1., -4., 1.],\n                          [0.,  1., 0.]], dtype=tf.float32)\n    kernel = tf.reshape(kernel, [3, 3, 1, 1])\n    kernel = tf.repeat(kernel, repeats=3, axis=2)  # Apply to each RGB channel\n    return kernel\n\ndef apply_laplacian(img):\n    kernel = laplacian_filter()\n    return tf.nn.conv2d(img, kernel, strides=1, padding='SAME')\n\ndef high_freq_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    lap_true = apply_laplacian(y_true)\n    lap_pred = apply_laplacian(y_pred)\n    \n    return tf.reduce_mean(tf.abs(lap_true - lap_pred))","metadata":{"_uuid":"4fdea234-d0f8-4127-bb8d-6852dc57e23d","_cell_guid":"05205190-6090-49b3-b04b-8f852207e9a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(images,epoch):\n    with tf.GradientTape() as vq_tape, tf.GradientTape() as disc_tape:\n        vqvae_output = vqvae(images,training=True)\n        disc_true = discriminator(images,training=True)\n        disc_false = discriminator(vqvae_output,training=True)\n        \n        disc_loss = discriminator_loss(disc_true,disc_false)\n        gen_loss = generator_loss(disc_false)\n\n        perceptual_loss = jom_loss(images,vqvae_output)\n        quantizer_loss = tf.reduce_mean(vqvae.losses)\n        pixel_wise_loss = tf.reduce_mean(tf.abs(images-vqvae_output))\n        hfl = high_freq_loss(images,vqvae_output)\n        \n        gen_weight = tf.minimum(0.4, tf.cast(epoch//5, tf.float32) * 0.05)\n        perceptual_weight = tf.minimum(0.5, tf.cast(epoch//5, tf.float32) * 0.01)      \n        total_gen_loss = pixel_wise_loss + quantizer_loss + gen_weight * gen_loss + perceptual_weight * perceptual_loss + 0.1 * hfl\n        \n        gradients_of_vqvae = vq_tape.gradient(total_gen_loss, vqvae.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n        vqvae_optimizer.apply_gradients(zip(gradients_of_vqvae, vqvae.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n        output_min = tf.reduce_min(vqvae_output)\n        output_max = tf.reduce_max(vqvae_output)\n        output_mean = tf.reduce_mean(vqvae_output)\n        return (\n            gen_loss, pixel_wise_loss, perceptual_loss, quantizer_loss, disc_loss,\n            tf.reduce_min(vqvae_output),  # Already reduced per-replica\n            tf.reduce_max(vqvae_output),\n            tf.reduce_mean(vqvae_output),\n            total_gen_loss,\n            hfl\n            )","metadata":{"_uuid":"df7d4b89-88b5-4208-a533-cd82bee41105","_cell_guid":"e18a4308-ce5b-4658-b0ae-a2d28d491952","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef distributed_train_step(batch, epoch):\n    return strategy.run(train_step, args=(batch, epoch))","metadata":{"_uuid":"a2cef881-f5e2-4225-8fd6-23eb9b94c630","_cell_guid":"ab8bae4a-5694-43ed-9a64-25a7c2422374","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\n\ndef run_custom_callback(data_sample, epoch, every_n_epochs, num_images=5):\n    current_epoch = epoch + 1\n    save_model = (current_epoch % 6 == 0)\n    show_plot = (current_epoch % every_n_epochs == 0)\n\n    # Save the model every 6 epochs\n\n\n    # Display the plot every `every_n_epochs`\n    if not show_plot:\n        return\n\n    clear_output(wait=True)\n\n    encoder = vqvae.get_layer(name='encoder')\n    vq_layer = vqvae.get_layer(name='quantizer')\n    codebook = vq_layer.embeddings\n    decoder = vqvae.get_layer(name='decoder')\n\n    if save_model:\n        models[epoch+1] = [encoder,decoder,codebook] \n\n    z = encoder(data_sample)  # shape: (B, H, W, embedding_dim)\n    z_flat = tf.reshape(z, [-1, z.shape[-1]])  # Flatten to (B*H*W, embedding_dim)\n    mean = tf.reduce_mean(z_flat, axis=0)       # Shape: (D,)\n    stddev = tf.math.reduce_std(z_flat, axis=0)  # Shape: (D,)\n\n    distances = (\n        tf.reduce_sum(z_flat ** 2, axis=1, keepdims=True)\n        - 2 * tf.matmul(z_flat, codebook, transpose_b=True)\n        + tf.reduce_sum(codebook ** 2, axis=1)\n    )\n\n    encoding_indices = tf.argmin(distances, axis=1)\n    print(\"Unique encoding indices used:\", len(np.unique(encoding_indices.numpy())))\n\n    reconstructions = vqvae(data_sample, training=False).numpy()\n    reconstructions = np.clip(reconstructions, 0., 1.)\n\n    fig, axes = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n    for i in range(num_images):\n        axes[0, i].imshow(data_sample[i])\n        axes[0, i].axis('off')\n        axes[1, i].imshow(reconstructions[i])\n        axes[1, i].axis('off')\n\n    plt.suptitle(f\"Epoch {current_epoch}: Top = Input | Bottom = Reconstruction\")\n    plt.tight_layout()\n\n    # Save the plot only if model is also saved (i.e., every 6 epochs)\n    if save_model:\n        plot_filename = f'{current_epoch}_Epoch_Reconstructions.png'\n        plt.savefig(plot_filename)\n        print(f\"Reconstruction plot saved as {plot_filename}\")\n\n    plt.show()","metadata":{"_uuid":"edb53dc9-df92-4f27-8390-87b7b6f90e9b","_cell_guid":"1a230b4d-3cce-439e-873c-ff52a485405e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = []\nfor i in os.listdir('/kaggle/input/flickr8k/Images')[10:15]:\n    img = Image.open('/kaggle/input/flickr8k/Images/' + i)\n    img = img.resize((128,128))\n    img = np.asarray(img)/255.0\n    test.append(img)","metadata":{"_uuid":"541e78af-fc89-463d-acd3-da8776193339","_cell_guid":"cb8fd0d2-ddda-483d-bf73-e172624756e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 99 # int(input('ENTER THEM DIGITS : '))\n\nmodels={}\ndef train_model(train_dataset, epochs, data_sample, every_n_epochs=3):\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        total_gen_loss = 0\n        total_pixel_loss = 0\n        total_quant_loss = 0\n        total_disc_loss = 0\n        total_percep_loss = 0\n        total_total_gen_loss = 0\n        total_hfl_loss = 0\n        steps = 0\n\n        for batch in tqdm(train_dataset, desc=\"Training\", leave=False):\n            per_replica_output = distributed_train_step(batch, epoch+1)\n        \n            gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[0], axis=None)\n            pixel_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[1], axis=None)\n            perceptual_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[2], axis=None)\n            quantizer_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[3], axis=None)\n            disc_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[4], axis=None)\n            \n            out_min = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[5], axis=None)\n            out_max = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[6], axis=None)\n            out_mean = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[7], axis=None)# Accumulate\n            step_total_gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[8], axis=None)# Accumulate\n            hfl_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[9], axis=None)# Accumulate\n\n            total_gen_loss += gen_loss.numpy()\n            total_pixel_loss += pixel_loss.numpy()\n            total_quant_loss += quantizer_loss.numpy()\n            total_disc_loss += disc_loss.numpy()\n            total_percep_loss += perceptual_loss.numpy()\n            total_total_gen_loss += step_total_gen_loss.numpy()\n            total_hfl_loss += hfl_loss.numpy()\n            steps += 1\n\n\n        print(f\"Epoch {epoch + 1} Summary:\")\n        print(f\"  Generator Loss: {total_gen_loss / steps:.4f}\")\n        print(f\"  Pixel-wise Loss: {total_pixel_loss / steps:.4f}\")\n        print(f\"  Perceptual Loss: {total_percep_loss / steps:.4f}\")\n        print(f\"  Quantizer Loss: {total_quant_loss / steps:.4f}\")\n        print(f\"  Discriminator Loss: {total_disc_loss / steps:.4f}\")\n        print(f\"  Total HFL Loss: {total_hfl_loss / steps:.4f}\")\n        print(f\"Output Stats — min: {out_min.numpy()}, max: {out_max.numpy()}, mean: {out_mean.numpy()}\")\n        print(f\"  Total Gen Loss: {total_total_gen_loss / steps:.4f}\")\n\n        # Run callback\n        \n        run_custom_callback(data_sample, epoch, every_n_epochs)\n        \nis_training = int(input('1 for yes \\n 0 for no \\n Do you Want to Train ? '))\n\nif is_training:\n    train_model(vqvae_train, EPOCHS,np.array(test))\n\nelse:\n    \n    '''model = tf.keras.models.load_model('/kaggle/input/vqganbest_one/tensorflow2/default/1/84_epoch_model.keras', custom_objects={\n            'codebook_kl_loss': codebook_kl_loss,\n            'VectorQuantizer': VectorQuantizer,\n            'SelfAttention': SelfAttention})'''\n    encoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts2/vqgan_epoch_84/epoch_84_encoder.keras',custom_objects={'SelfAttention': SelfAttention})\n    decoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts2/vqgan_epoch_84/epoch_84_decoder.keras',custom_objects={'SelfAttention': SelfAttention})\n    codebook = np.load('/kaggle/input/vqgan-parts2/vqgan_epoch_84/epoch_84_codebook.npy')","metadata":{"_uuid":"f5585479-afc8-4110-a334-5f9aea4ab8de","_cell_guid":"a3909e07-4dd1-4756-884b-e59bc6a296eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport math\n\ndef display_pngs_from_folder(folder_path):\n    # Get all PNG files from the folder\n    png_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n    png_files.sort()  # Optional: sort by filename\n\n    num_images = len(png_files)\n    if num_images == 0:\n        print(\"No PNG files found in the folder.\")\n        return\n\n    # Calculate grid size (e.g., 3 columns)\n    cols = 3\n    rows = math.ceil(num_images / cols)\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n    axes = axes.flatten()  # Flatten in case of single row or column\n\n    for i, png_file in enumerate(png_files):\n        img_path = os.path.join(folder_path, png_file)\n        img = mpimg.imread(img_path)\n        axes[i].imshow(img)\n        axes[i].set_title(png_file)\n        axes[i].axis('off')\n\n    # Hide any empty subplots\n    for j in range(i + 1, len(axes)):\n        axes[j].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_pngs_from_folder('/kaggle/working/')","metadata":{"_uuid":"05bce9cb-95c9-4b5c-9b51-8859335eed58","_cell_guid":"c1355b5e-180a-4848-a912-980b5f03b111","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''models[6][2].numpy()'''","metadata":{"_uuid":"1817b217-5f51-4f3c-bef7-9ae640064004","_cell_guid":"7a57ea55-7429-43ef-9e16-c80a6c91fed2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''for i in [66,84]:\n    models[i][0].save(f'epoch_{i}_encoder.keras')\n    models[i][1].save(f'epoch_{i}_decoder.keras')\n    np.save(f'epoch_{i}_codebook.npy', models[i][2].numpy())\n'''","metadata":{"_uuid":"519ff159-2a6f-40be-b346-883308b0fe60","_cell_guid":"d9f4a337-48e7-44a9-9708-746ee4c91b2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_codebook_indices(imgs,codebook):\n    inputs = encoder(imgs)\n    embeddings=codebook\n    embedding_dim = tf.shape(codebook)[-1]\n    input_shape = tf.shape(inputs)\n    flat_inputs = tf.reshape(inputs, [-1, embedding_dim])\n    flat_inputs = tf.cast(flat_inputs, dtype=embeddings.dtype)\n    \n        # Compute distances to embedding vectors\n    '''distances = (\n            tf.reduce_sum(flat_inputs ** 2, axis=1, keepdims=True)\n            - 2 * tf.matmul(flat_inputs, self.embeddings, transpose_b=True)\n            + tf.reduce_sum(self.embeddings ** 2, axis=1)\n        )'''\n    flat_inputs_norm = tf.nn.l2_normalize(flat_inputs, axis=1)  # [N, D]\n    embeddings_norm = tf.nn.l2_normalize(embeddings, axis=1)  # [K, D]\n        \n        # Cosine similarity = dot product of normalized vectors\n    cosine_sim = tf.matmul(flat_inputs_norm, embeddings_norm, transpose_b=True)  # [N, K]\n        \n        # Convert similarity to distance (cosine distance = 1 - cosine similarity)\n    distances = 1.0 - cosine_sim\n    # Find nearest embedding index\n    encoding_indices = tf.argmin(distances, axis=1)\n    return tf.reshape(encoding_indices, input_shape[:-1])\n\ndef get_embeddings(indices, codebook):\n    # Flatten indices to 1D or 2D if needed\n    flat_indices = tf.reshape(indices, [-1])\n    \n    # Lookup embeddings\n    flat_embeddings = tf.nn.embedding_lookup(codebook, flat_indices)  # [B*H*W, D]\n\n    # Reshape back to match original spatial structure + D\n    out_shape = tf.concat([tf.shape(indices), [tf.shape(codebook)[-1]]], axis=0)\n    return tf.reshape(flat_embeddings, out_shape)","metadata":{"_uuid":"634eb42b-a5c1-4cca-b40c-b47e75ec954e","_cell_guid":"b40f03ae-128c-4ef0-863d-79f752b79272","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(vqvae_train))\n\n# Convert PerReplica to a real tensor — pick one replica (usually 0 is fine)\nsample_batch = strategy.experimental_local_results(batch)[0]\n\n# If it's (input, label) format\ntst = sample_batch[4].numpy()\n\n\nplt.imshow(tst)\nplt.show()\nimg = np.expand_dims(tst,0)\nout = decoder(get_embeddings(get_codebook_indices(img,codebook),codebook))\n#out = vqvae(img)\nplt.imshow(out[0])\nplt.show()","metadata":{"_uuid":"294eb463-b983-4446-823c-4b1f9dfe1d15","_cell_guid":"81595fd7-b202-42a0-b6a4-644ed5c34bb2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open('/kaggle/input/monet2photo/trainB/2013-11-08 16_45_24.jpg')\nimg = img.resize((128,128))\nimg = np.asarray(img)/255.0\nplt.imshow(img)\nplt.show()\nimg = np.expand_dims(img,0)\nout = decoder(get_embeddings(get_codebook_indices(img,codebook),codebook))\nplt.imshow(out[0])\nplt.show()","metadata":{"_uuid":"9358dd47-ad4d-47ef-9344-554799cc792b","_cell_guid":"6d3f4e79-4836-42bf-ac28-aa4d2470aeac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total model losses:\", vqvae.losses)","metadata":{"_uuid":"4ef68d4a-2696-4a26-8ce5-a611a3ad0c34","_cell_guid":"162b3428-3967-4455-b6cb-3de6e53b5627","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_loading = int(input('DO YOU WANT TO LOAD THE DATA \\n 1 for Yes \\n 0 for No \\n Enter : '))\nprint(is_loading)\nif is_loading:\n    from tqdm import tqdm  # for progress bar\n    \n    image_paths = []\n    captions = []\n    \n    BATCH_SIZE = 128\n    \n    df = pd.read_csv('/kaggle/input/flickr30k/captions.txt')\n    # Step 1: Collect all image paths and captions\n    for row in df.itertuples(index=False):\n        image_paths.append('/kaggle/input/flickr30k/Images/' + row.image)\n        captions.append(row.caption)\n    \n    tokens_list = []\n    \n    # Step 2: Process images in batches\n    for i in tqdm(range(0, len(image_paths), BATCH_SIZE)):\n        batch_paths = image_paths[i:i+BATCH_SIZE]\n        \n        # Step 3: Preprocess each image\n        batch_images = []\n        for path in batch_paths:\n            img = Image.open(path).convert(\"RGB\")\n            img = img.resize((WIDTH, HEIGHT))\n            img = np.asarray(img) / 255.0\n            batch_images.append(img)\n    \n        # Step 4: Stack and convert to tensor\n        batch_images = tf.convert_to_tensor(batch_images, dtype=tf.float32)\n    \n        # Step 5: Pass batch to model\n        tokens = get_codebook_indices(batch_images,codebook)  # shape: (B, 8, 8)\n        tokens_list.append(tokens)\n    \n    # Step 6: Final shaping\n    tokens_all = tf.concat(tokens_list, axis=0)  # (N, 8, 8)\n    tokens_seq = tf.reshape(tokens_all, [tokens_all.shape[0], -1])  # (N, 64)\nelse:\n    captions = np.load('/kaggle/input/vqgan-parts2/vqgan_epoch_84/Captions.npy')\n    tokens_seq = np.load('/kaggle/input/vqgan-parts2/vqgan_epoch_84/Tokens_seq.npy').astype(np.int32)","metadata":{"_uuid":"ead0d3b9-f041-4790-af13-038c59360aea","_cell_guid":"3d2ad114-3c5b-41bc-b647-375ccfdd2301","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\nimport re\n\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 16000\nsequence_length = 45\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\n\nvectorizer.adapt(captions)\ntext_vect = tf.cast(vectorizer(captions),tf.int32)","metadata":{"_uuid":"7abfec3a-b164-49b2-a9a8-990d7fafa476","_cell_guid":"30a68179-ba3d-4088-aefd-dab55d4b84e3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(tokens_seq))\nprint(np.shape(text_vect))","metadata":{"_uuid":"81c3fbca-a405-4419-acb3-b3f0c9dc3fed","_cell_guid":"e72aff75-034d-4dbf-adee-842d1edcf6b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_tokens = tf.cast(tf.fill([len(text_vect), 1], num_embeddings),tf.int32)\ndecoder_tokens = tf.concat([start_tokens, tokens_seq], axis=1)","metadata":{"_uuid":"95c0a9dd-6b90-472e-9d4a-70063d4ecf9f","_cell_guid":"dd7d7171-6d1e-477f-afc3-8d96f694c599","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(decoder_tokens))","metadata":{"_uuid":"34887f96-bffa-4c8b-9998-93bb63db0c0f","_cell_guid":"23b6298a-1338-455f-a78d-4775fc43c22d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ex = tokens_seq\nprint(len(np.unique(ex)))","metadata":{"_uuid":"48e31b99-cac3-4045-8dfc-c930eddd0a70","_cell_guid":"37ea6df6-3639-4446-8f99-7bec4a07bb06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.min(tokens_seq)","metadata":{"_uuid":"ba89ced4-dccc-435a-a525-6851e92bd80d","_cell_guid":"958a22ee-6fa1-4206-9ece-324c17a2a022","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(np.unique(tokens_seq[9]))","metadata":{"_uuid":"7088bccd-d6e5-4230-908a-c68163647ad8","_cell_guid":"add5b155-a3c6-4e8b-8e1b-34dbc05107e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = tf.random.uniform(shape=(1, 16, 16), minval=100, maxval=254, dtype=tf.int32)\nflat_indices = tf.reshape(random_indices, [-1])\nquantized_vectors = tf.gather(codebook, flat_indices)\nquantized_vectors = tf.reshape(quantized_vectors, (1, 16, 16\n                                                   , embedding_dim))\n\n# Decode to image\ngenerated_image = decoder(quantized_vectors)\n# Display\nimport matplotlib.pyplot as plt\nplt.imshow(generated_image[0].numpy())\nplt.axis('off')\nplt.title('Random Image from VQ-VAE Codebook')\nplt.show()","metadata":{"_uuid":"448221fd-13a1-445e-b5bc-15925efb4591","_cell_guid":"77512927-b406-44b2-8dc7-ea511fdc2138","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef transformer_encoder(x, mask, embed_dim, num_heads, ff_dim, dropout=0.1):\n    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x, attention_mask=mask)\n    attn_output = layers.Dropout(dropout)(attn_output)\n    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n\n    ffn = layers.Dense(ff_dim, activation=\"relu\")(out1)\n    ffn = layers.Dense(embed_dim)(ffn)\n    ffn = layers.Dropout(dropout)(ffn)\n    return layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)\n\n\ndef transformer_decoder(x, enc_out, embed_dim, num_heads, ff_dim, dropout=0.1):\n    attn1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x, use_causal_mask=True)\n    attn1 = layers.Dropout(dropout)(attn1)\n    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn1)\n\n    attn2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(out1, enc_out)\n    attn2 = layers.Dropout(dropout)(attn2)\n    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + attn2)\n\n    ffn = layers.Dense(ff_dim, activation=\"relu\")(out2)\n    ffn = layers.Dense(embed_dim)(ffn)\n    ffn = layers.Dropout(dropout)(ffn)\n    return layers.LayerNormalization(epsilon=1e-6)(out2 + ffn)","metadata":{"_uuid":"2e16ee8a-138a-4326-98ad-2e7631427ea9","_cell_guid":"3e0bc54a-1dfc-494c-80de-aa6eda06ed94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        seq_len = tf.shape(x)[1]\n        positions = tf.range(start=0, limit=seq_len, delta=1)\n        positions = self.pos_emb(positions)\n        return self.token_emb(x) + positions","metadata":{"_uuid":"b637023a-b763-4d36-a03f-1e20690ee19d","_cell_guid":"9134b311-9146-4e3c-8dfd-2504a9acc1e8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}