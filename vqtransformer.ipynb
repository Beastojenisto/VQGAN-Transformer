{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":1567182,"sourceType":"datasetVersion","datasetId":925704},{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179},{"sourceId":12456683,"sourceType":"datasetVersion","datasetId":7857754},{"sourceId":12744948,"sourceType":"datasetVersion","datasetId":8013533}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from transformers import AutoTokenizer, TFAutoModel\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random","metadata":{"_uuid":"f7e4ac21-a077-4c6f-b5cf-372c87196521","_cell_guid":"b6f2f5f0-a68b-4790-a48d-28a880347e8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver('local')\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    print('die')\n    strategy = tf.distribute.get_strategy()","metadata":{"_uuid":"641976da-272a-4902-8282-cc5293b1baa5","_cell_guid":"076c4357-085c-4f6d-ac97-ae9528e221a1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr30k/captions.txt')","metadata":{"_uuid":"2de2aa7d-45c1-45a6-8c13-9b51a832f82f","_cell_guid":"12af3335-94c1-4552-8507-3ecfebaba504","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['image_name']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''df['image_name'] = df['image_name'].apply(lambda x: x.split('#')[0])'''\ndf = df.drop_duplicates(subset='image_name', keep='first')[:256*64]","metadata":{"_uuid":"28b37770-1780-4f69-b055-f5a7619cbdb9","_cell_guid":"65b3b499-e143-42ab-b4d6-aa2ec2fa403c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paths = df['image_name'].values\n'''captions = df['caption'].values'''","metadata":{"_uuid":"c5d7e30c-ad64-4bda-870d-f921a81f06ef","_cell_guid":"d3674c1d-7f0c-497f-ad5c-ed3f5491469e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = []\nHEIGHT,WIDTH = 128,128\nfor i in paths:\n    img = Image.open('/kaggle/input/flickr30k/flickr30k_images/'+i).convert(\"RGB\")\n    img = img.resize((HEIGHT,WIDTH))\n    img = np.asarray(img)/255.0\n    images.append(img)\n\n\nimages = np.array(images)  # Make sure images is a NumPy array\n\nprint(\"Images shape:\", images.shape)\nprint(\"Images dtype:\", images.dtype)\nprint(\"Images min:\", images.min())  # Use numpy's min method\nprint(\"Images max:\", images.max())  # Use numpy's max method\nprint(\"Images mean:\", images.mean())","metadata":{"_uuid":"14be7143-355d-4899-a0e2-e19cc977555d","_cell_guid":"3dd1d681-245d-4f9f-a9ca-44f44f9fb848","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vqvae_train = tf.data.Dataset.from_tensor_slices(images)\nvqvae_train = vqvae_train.map(lambda x: tf.cast(x, tf.float32), num_parallel_calls=tf.data.AUTOTUNE)\n'''vqvae_train = vqvae_train.shuffle(1000,seed=seed)'''\nvqvae_train = vqvae_train.batch(256, drop_remainder=True)  # Drop the last incomplete batch\nvqvae_train = vqvae_train.prefetch(tf.data.AUTOTUNE).cache()\n\nvqvae_train = strategy.experimental_distribute_dataset(vqvae_train)","metadata":{"_uuid":"150c8d8b-61a8-4096-9e4f-622c841f8cf2","_cell_guid":"5771ec35-a4e3-4d58-bbe8-ed031b176e3f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom IPython.display import clear_output\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\n\n# Hyperparameters\nembedding_dim = 256\nnum_embeddings = 256\ninput_shape = (HEIGHT, WIDTH, 3)\n\n\nroshnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet',input_shape=(224,224,3))\nroshnet.trainable = False\nname = 'conv3_block4_out'\noutputs = roshnet.get_layer(name).output\nroshnet_model = tf.keras.Model(inputs=roshnet.input, outputs=outputs)\n\n@tf.function\ndef codebook_kl_loss(encodings, num_embeddings, eps=1e-10):\n    # Flatten the encoding indices\n    encodings = tf.reshape(encodings, [-1])\n\n    # One-hot encode the indices\n    one_hot = tf.one_hot(encodings, depth=num_embeddings)\n\n    # Histogram\n    usage = tf.reduce_sum(one_hot, axis=0)\n\n    # Normalize\n    usage = usage / (tf.reduce_sum(usage) + eps)\n\n    # KL divergence to uniform prior (1 / num_embeddings)\n    kl = tf.reduce_sum(usage * tf.math.log((usage + eps) * num_embeddings))\n    return kl\ndef orthogonal_loss(codebook):\n    E = tf.nn.l2_normalize(codebook,axis=1)\n    sim_matrix = tf.matmul(E, E, transpose_b=True)\n    off_diag = sim_matrix - tf.linalg.diag(tf.linalg.diag_part(sim_matrix))\n    return tf.reduce_sum(tf.square(off_diag))\n    \ndef codebook_entropy_loss(indices, num_embeddings):\n    counts = tf.math.bincount(indices, minlength=num_embeddings, maxlength=num_embeddings, dtype=tf.float32)\n    probs = counts / (tf.reduce_sum(counts) + 1e-8)\n\n    entropy = -tf.reduce_sum(probs * tf.math.log(probs + 1e-8))\n    return -entropy \n\nclass VectorQuantizer(layers.Layer):\n    def __init__(self, num_embeddings, embedding_dim,  **kwargs):\n        super().__init__(**kwargs)\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        #self.last_used_indices = None\n        # Create the embeddings (the codebook)\n        self.latent_basis = self.add_weight(\n            shape=(self.num_embeddings, self.embedding_dim),\n            initializer=tf.random_uniform_initializer(),\n            trainable=True,\n            name=\"embeddings\",\n        )\n        self.dense_proj = tf.keras.layers.Dense(self.embedding_dim)\n\n    def call(self, inputs):\n        embeddings = self.dense_proj(self.latent_basis)\n        input_shape = tf.shape(inputs)\n        flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])\n        flat_inputs = tf.cast(flat_inputs, dtype=embeddings.dtype)\n        noise = tf.random.normal(shape=tf.shape(flat_inputs), mean=0.0, stddev=0.25)\n        flat_inputs += noise\n    \n        # Compute distances to embedding vectors\n        '''distances = (\n            tf.reduce_sum(flat_inputs ** 2, axis=1, keepdims=True)\n            - 2 * tf.matmul(flat_inputs, self.embeddings, transpose_b=True)\n            + tf.reduce_sum(self.embeddings ** 2, axis=1)\n        )'''\n        flat_inputs_norm = tf.nn.l2_normalize(flat_inputs, axis=1)  # [N, D]\n        embeddings_norm = tf.nn.l2_normalize(embeddings, axis=1)  # [K, D]\n        \n        # Cosine similarity = dot product of normalized vectors\n        cosine_sim = tf.matmul(flat_inputs_norm, embeddings_norm, transpose_b=True)  # [N, K]\n        \n        # Convert similarity to distance (cosine distance = 1 - cosine similarity)\n        distances = 1.0 - cosine_sim\n        # Find nearest embedding index\n        encoding_indices = tf.argmin(distances, axis=1)\n    \n        # Get quantized vectors\n        quantized = tf.gather(embeddings, encoding_indices)\n        quantized = tf.reshape(quantized, input_shape)\n    \n        # Compute losses\n        e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs) ** 2)\n        q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs)) ** 2)\n\n        ortho_loss = orthogonal_loss(embeddings)\n    \n        # Entropy loss (you must implement or import this)\n\n        # entropy_loss = codebook_entropy_loss(encoding_indices, num_embeddings = self.num_embeddings)\n        \n        kldiv_loss = codebook_kl_loss(encoding_indices, num_embeddings=self.num_embeddings)\n   \n        # Total loss\n        loss = q_latent_loss + 0.25 * e_latent_loss + 0.3 * kldiv_loss + 0.005 * ortho_loss\n        self.add_loss(loss) \n    \n        # Straight-through estimator\n        quantized = inputs + tf.stop_gradient(quantized - inputs)\n    \n        return quantized  # Optionally, also return encoding_indices\n\n    def get_codebook(self):\n        codebook = self.dense_proj(self.latent_basis)\n        return codebook\n\ndef residual_block(x, filters):\n    skip = x\n    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n\n    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.add([x, skip])\n    x = layers.ReLU()(x)\n    return x\n\n# Encoder\nfrom tensorflow.keras.layers import Layer, Conv2D, Add\nimport tensorflow as tf\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer, Conv2D, Add\nfrom tensorflow.keras.utils import register_keras_serializable\n\n@register_keras_serializable(package=\"Custom\")\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.filters = input_shape[-1]\n        self.f = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.g = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.h = Conv2D(self.filters, kernel_size=1, padding='same')\n        super().build(input_shape)\n\n    def call(self, x):\n        f = self.f(x)  # (B, H, W, C//8)\n        g = self.g(x)\n        h = self.h(x)  # (B, H, W, C)\n\n        shape_f = tf.shape(f)\n        B, H, W = shape_f[0], shape_f[1], shape_f[2]\n\n        f_flat = tf.reshape(f, [B, H * W, self.filters // 8])\n        g_flat = tf.reshape(g, [B, H * W, self.filters // 8])\n        h_flat = tf.reshape(h, [B, H * W, self.filters])\n\n        beta = tf.nn.softmax(tf.matmul(f_flat, g_flat, transpose_b=True), axis=-1)  # (B, N, N)\n\n        o = tf.matmul(beta, h_flat)  # (B, N, C)\n        o = tf.reshape(o, [B, H, W, self.filters])\n\n        return Add()([x, o])  # Residual connection\n\n    def get_config(self):\n        config = super().get_config()\n        # If you have custom arguments in __init__, add them here\n        return config\n\n\ndef get_encoder():\n    inputs = keras.Input(shape=input_shape)\n\n    # Downsample 1: 128x128 → 64x64\n    x = layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(inputs)\n    for _ in range(2): x = residual_block(x, 256)\n\n    # Downsample 2: 64x64 → 32x32\n    x = layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 512)\n\n    # Downsample 3: 32x32 → 16x16\n    x = layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x)\n    for _ in range(3): x = residual_block(x, 512)\n    \n    x = SelfAttention()(x)\n    \n    x = layers.Conv2D(embedding_dim, 1, padding='same')(x)\n    # x = layers.LayerNormalization()(x) \n    # Final latent\n    return keras.Model(inputs, x, name=\"encoder\")\n\n\n# Decoder\ndef get_decoder():\n    inputs = keras.Input(shape=(16, 16, embedding_dim))  # Start from encoded shape\n\n    x = layers.UpSampling2D(size=(2, 2))(inputs)\n    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n    for _ in range(3): x = residual_block(x, 512)\n\n    x = SelfAttention()(x)\n    \n    x = layers.UpSampling2D(size=(2, 2))(x)\n    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 512)\n\n    x = layers.UpSampling2D(size=(2, 2))(x)\n    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n    for _ in range(2): x = residual_block(x, 256)\n\n    x = layers.Conv2D(3, 1, padding='same', activation='sigmoid')(x)\n\n    return keras.Model(inputs, x, name=\"decoder\")\n\ndef jom_loss(y_true, y_pred):\n    # Perceptual loss (L1 on VGG features)\n    y_true_resized = tf.image.resize(y_true, (224, 224))*255\n    y_pred_resized = tf.image.resize(y_pred, (224, 224))*255\n\n    y_true_roshnet = tf.keras.applications.resnet50.preprocess_input(y_true_resized)\n    y_pred_roshnet = tf.keras.applications.resnet50.preprocess_input(y_pred_resized)\n    roshnet_true = tf.stop_gradient(roshnet_model(y_true_roshnet))\n    roshnet_pred = roshnet_model(y_pred_roshnet)\n    \n    perceptual_loss = tf.reduce_mean(tf.square(roshnet_true - roshnet_pred))\n    return perceptual_loss\n    \nwith strategy.scope():\n    encoder = get_encoder()\n    decoder = get_decoder()\n    inputs = keras.Input(shape=input_shape)\n    z = encoder(inputs)\n    quantized = VectorQuantizer(num_embeddings, embedding_dim,name='quantizer')(z)\n    reconstructions = decoder(quantized)\n    \n    # This model has more neurons than my brain\n    vqvae = keras.Model(inputs, reconstructions, name=\"vqvae\")\n    \n    vqvae_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\nsample_batch = np.array(images[:5])  # Get 10 sample images for monitoring\n\n# Compile and train\nvqvae.summary()","metadata":{"_uuid":"40d043a1-8d4e-49db-ade5-bc9807b2c896","_cell_guid":"6d2386d1-0b7f-41ea-b517-70636e398d0f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_discriminator(input_shape=(128, 128, 3)):\n    inputs = keras.Input(shape=input_shape)\n\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)  # 128→64\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)       # 64→32\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)       # 32→16\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(512, 4, strides=2, padding='same')(x)       # 16→8\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Conv2D(1, 4, padding='valid')(x)  # output single scalar or patch (5x5 or so)\n    \n    # Flatten and output\n    x = layers.Flatten()(x)\n    outputs = layers.Activation('sigmoid')(x)  # probability real/fake\n\n    return keras.Model(inputs, outputs, name=\"discriminator\")\n    \nwith strategy.scope():\n    discriminator = get_discriminator()\n    \ndiscriminator.summary()","metadata":{"_uuid":"63d5b3bd-4827-459f-9fa3-db84fd872fb9","_cell_guid":"60963d0c-5a6e-4e16-afe4-9eb2577a3112","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ndef discriminator_loss(real,fake):\n    real_loss = loss(tf.ones_like(real)*0.9,real)\n    fake_loss = loss(tf.zeros_like(fake),fake)\n    total_loss = real_loss+fake_loss\n    return total_loss\n\ndef generator_loss(fake):\n    gen_loss = loss(tf.ones_like(fake),fake)\n    return gen_loss","metadata":{"_uuid":"b0f8d724-9d22-4d93-97b8-6b1f8d43a951","_cell_guid":"cc2cb1bb-f63c-496f-8cbd-3518b8e33ef8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef laplacian_filter():\n    # 3x3 Laplacian kernel\n    kernel = tf.constant([[0.,  1., 0.],\n                          [1., -4., 1.],\n                          [0.,  1., 0.]], dtype=tf.float32)\n    kernel = tf.reshape(kernel, [3, 3, 1, 1])\n    kernel = tf.repeat(kernel, repeats=3, axis=2)  # Apply to each RGB channel\n    return kernel\n\ndef apply_laplacian(img):\n    kernel = laplacian_filter()\n    return tf.nn.conv2d(img, kernel, strides=1, padding='SAME')\n\ndef high_freq_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n\n    lap_true = apply_laplacian(y_true)\n    lap_pred = apply_laplacian(y_pred)\n    \n    return tf.reduce_mean(tf.abs(lap_true - lap_pred))","metadata":{"_uuid":"4fdea234-d0f8-4127-bb8d-6852dc57e23d","_cell_guid":"05205190-6090-49b3-b04b-8f852207e9a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step(images,epoch):\n    with tf.GradientTape() as vq_tape, tf.GradientTape() as disc_tape:\n        vqvae_output = vqvae(images,training=True)\n        disc_true = discriminator(images,training=True)\n        disc_false = discriminator(vqvae_output,training=True)\n        \n        disc_loss = discriminator_loss(disc_true,disc_false)\n        gen_loss = generator_loss(disc_false)\n\n        perceptual_loss = jom_loss(images,vqvae_output)\n        quantizer_loss = tf.reduce_mean(vqvae.losses)\n        pixel_wise_loss = tf.reduce_mean(tf.abs(images-vqvae_output))\n        hfl = high_freq_loss(images,vqvae_output)\n        \n        gen_weight = tf.minimum(0.4, tf.cast(epoch//5, tf.float32) * 0.05)\n        perceptual_weight = tf.minimum(0.5, tf.cast(epoch//5, tf.float32) * 0.01)      \n        total_gen_loss = pixel_wise_loss + quantizer_loss + gen_weight * gen_loss + perceptual_weight * perceptual_loss + 0.1 * hfl\n        \n        gradients_of_vqvae = vq_tape.gradient(total_gen_loss, vqvae.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n        vqvae_optimizer.apply_gradients(zip(gradients_of_vqvae, vqvae.trainable_variables))\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n        output_min = tf.reduce_min(vqvae_output)\n        output_max = tf.reduce_max(vqvae_output)\n        output_mean = tf.reduce_mean(vqvae_output)\n        return (\n            gen_loss, pixel_wise_loss, perceptual_loss, quantizer_loss, disc_loss,\n            tf.reduce_min(vqvae_output),  # Already reduced per-replica\n            tf.reduce_max(vqvae_output),\n            tf.reduce_mean(vqvae_output),\n            total_gen_loss,\n            hfl\n            )","metadata":{"_uuid":"df7d4b89-88b5-4208-a533-cd82bee41105","_cell_guid":"e18a4308-ce5b-4658-b0ae-a2d28d491952","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef distributed_train_step(batch, epoch):\n    return strategy.run(train_step, args=(batch, epoch))","metadata":{"_uuid":"a2cef881-f5e2-4225-8fd6-23eb9b94c630","_cell_guid":"ab8bae4a-5694-43ed-9a64-25a7c2422374","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef plot_codebook_similarity(codebook, title=\"Codebook Cosine Similarity\"):\n    \"\"\"\n    codebook: Tensor of shape [num_codes, dim]\n    \"\"\"\n    codebook = tf.convert_to_tensor(codebook)\n\n    # Normalize for cosine similarity\n    norm_e = tf.nn.l2_normalize(codebook, axis=-1)\n    sim_matrix = tf.matmul(norm_e, norm_e, transpose_b=True)\n\n    # Plot heatmap\n    plt.figure(figsize=(6, 6))\n    plt.imshow(sim_matrix.numpy(), cmap=\"viridis\", interpolation=\"nearest\")\n    plt.colorbar(label=\"Cosine Similarity\")\n    plt.title(title)\n    plt.xlabel(\"Code Index\")\n    plt.ylabel(\"Code Index\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_similarity_histogram(codebook):\n    norm_e = tf.nn.l2_normalize(codebook, axis=-1)\n    sim_matrix = tf.matmul(norm_e, norm_e, transpose_b=True)\n\n    # remove diagonal self-similarities\n    mask = ~tf.eye(sim_matrix.shape[0], dtype=tf.bool)\n    sims = tf.boolean_mask(sim_matrix, mask)\n\n    plt.hist(sims.numpy(), bins=50, density=True, alpha=0.7)\n    plt.title(\"Distribution of Codebook Cosine Similarities\")\n    plt.xlabel(\"Cosine Similarity\")\n    plt.ylabel(\"Density\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\n\ndef run_custom_callback(data_sample, epoch, every_n_epochs, num_images=5):\n    current_epoch = epoch + 1\n    save_model = (current_epoch % 6 == 0)\n    show_plot = (current_epoch % every_n_epochs == 0)\n\n    # Save the model every 6 epochs\n\n\n    # Display the plot every `every_n_epochs`\n    if not show_plot:\n        return\n\n    clear_output(wait=True)\n\n    encoder = vqvae.get_layer(name='encoder')\n    vq_layer = vqvae.get_layer(name='quantizer')\n    codebook = vq_layer.get_codebook()\n    decoder = vqvae.get_layer(name='decoder')\n\n    if save_model:\n        models[epoch+1] = [encoder,decoder,codebook] \n\n    z = encoder(data_sample)  # shape: (B, H, W, embedding_dim)\n    z_flat = tf.reshape(z, [-1, z.shape[-1]])  # Flatten to (B*H*W, embedding_dim)\n    mean = tf.reduce_mean(z_flat, axis=0)       # Shape: (D,)\n    stddev = tf.math.reduce_std(z_flat, axis=0)  # Shape: (D,)\n\n    distances = (\n        tf.reduce_sum(z_flat ** 2, axis=1, keepdims=True)\n        - 2 * tf.matmul(z_flat, codebook, transpose_b=True)\n        + tf.reduce_sum(codebook ** 2, axis=1)\n    )\n\n    encoding_indices = tf.argmin(distances, axis=1)\n    print(\"Unique encoding indices used:\", len(np.unique(encoding_indices.numpy())))\n\n    reconstructions = vqvae(data_sample, training=False).numpy()\n    reconstructions = np.clip(reconstructions, 0., 1.)\n\n    fig, axes = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n    for i in range(num_images):\n        axes[0, i].imshow(data_sample[i])\n        axes[0, i].axis('off')\n        axes[1, i].imshow(reconstructions[i])\n        axes[1, i].axis('off')\n\n    plt.suptitle(f\"Epoch {current_epoch}: Top = Input | Bottom = Reconstruction\")\n    plt.tight_layout()\n\n    # Save the plot only if model is also saved (i.e., every 6 epochs)\n    if save_model:\n        plot_filename = f'{current_epoch}_Epoch_Reconstructions.png'\n        plt.savefig(plot_filename)\n        print(f\"Reconstruction plot saved as {plot_filename}\")\n\n    plt.show()\n    plot_codebook_similarity(codebook)\n    plot_similarity_histogram(codebook)","metadata":{"_uuid":"edb53dc9-df92-4f27-8390-87b7b6f90e9b","_cell_guid":"1a230b4d-3cce-439e-873c-ff52a485405e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = []\nfor i in os.listdir('/kaggle/input/flickr8k/Images')[10:15]:\n    img = Image.open('/kaggle/input/flickr8k/Images/' + i)\n    img = img.resize((128,128))\n    img = np.asarray(img)/255.0\n    test.append(img)","metadata":{"_uuid":"541e78af-fc89-463d-acd3-da8776193339","_cell_guid":"cb8fd0d2-ddda-483d-bf73-e172624756e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 99 # int(input('ENTER THEM DIGITS : '))\n\nmodels={}\ndef train_model(train_dataset, epochs, data_sample, every_n_epochs=3):\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        total_gen_loss = 0\n        total_pixel_loss = 0\n        total_quant_loss = 0\n        total_disc_loss = 0\n        total_percep_loss = 0\n        total_total_gen_loss = 0\n        total_hfl_loss = 0\n        steps = 0\n\n        for batch in tqdm(train_dataset, desc=\"Training\", leave=False):\n            per_replica_output = distributed_train_step(batch, epoch+1)\n        \n            gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[0], axis=None)\n            pixel_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[1], axis=None)\n            perceptual_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[2], axis=None)\n            quantizer_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[3], axis=None)\n            disc_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[4], axis=None)\n            \n            out_min = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[5], axis=None)\n            out_max = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[6], axis=None)\n            out_mean = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[7], axis=None)# Accumulate\n            step_total_gen_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[8], axis=None)# Accumulate\n            hfl_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_output[9], axis=None)# Accumulate\n\n            total_gen_loss += gen_loss.numpy()\n            total_pixel_loss += pixel_loss.numpy()\n            total_quant_loss += quantizer_loss.numpy()\n            total_disc_loss += disc_loss.numpy()\n            total_percep_loss += perceptual_loss.numpy()\n            total_total_gen_loss += step_total_gen_loss.numpy()\n            total_hfl_loss += hfl_loss.numpy()\n            steps += 1\n\n\n        print(f\"Epoch {epoch + 1} Summary:\")\n        print(f\"  Generator Loss: {total_gen_loss / steps:.4f}\")\n        print(f\"  Pixel-wise Loss: {total_pixel_loss / steps:.4f}\")\n        print(f\"  Perceptual Loss: {total_percep_loss / steps:.4f}\")\n        print(f\"  Quantizer Loss: {total_quant_loss / steps:.4f}\")\n        print(f\"  Discriminator Loss: {total_disc_loss / steps:.4f}\")\n        print(f\"  Total HFL Loss: {total_hfl_loss / steps:.4f}\")\n        print(f\"Output Stats — min: {out_min.numpy()}, max: {out_max.numpy()}, mean: {out_mean.numpy()}\")\n        print(f\"  Total Gen Loss: {total_total_gen_loss / steps:.4f}\")\n\n        # Run callback\n        \n        run_custom_callback(data_sample, epoch, every_n_epochs)\n        \nis_training = int(input('1 for yes \\n 0 for no \\n Do you Want to Train ? '))\n\nif is_training:\n    train_model(vqvae_train, EPOCHS,np.array(test))\n\nelse:\n    \n    '''model = tf.keras.models.load_model('/kaggle/input/vqganbest_one/tensorflow2/default/1/84_epoch_model.keras', custom_objects={\n            'codebook_kl_loss': codebook_kl_loss,\n            'VectorQuantizer': VectorQuantizer,\n            'SelfAttention': SelfAttention})'''\n    with strategy.scope():\n        encoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts/vqgan_epoch_90/epoch_90_encoder.keras',custom_objects={'SelfAttention': SelfAttention})\n        decoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts/vqgan_epoch_90/epoch_90_decoder.keras',custom_objects={'SelfAttention': SelfAttention})\n        codebook = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_90/epoch_90_codebook.npy')\n        codebook = tf.convert_to_tensor(codebook, dtype=tf.float32)","metadata":{"_uuid":"f5585479-afc8-4110-a334-5f9aea4ab8de","_cell_guid":"a3909e07-4dd1-4756-884b-e59bc6a296eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport math\n\ndef display_pngs_from_folder(folder_path):\n    # Get all PNG files from the folder\n    png_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.png')]\n    png_files.sort()  # Optional: sort by filename\n\n    num_images = len(png_files)\n    if num_images == 0:\n        print(\"No PNG files found in the folder.\")\n        return\n\n    # Calculate grid size (e.g., 3 columns)\n    cols = 3\n    rows = math.ceil(num_images / cols)\n\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n    axes = axes.flatten()  # Flatten in case of single row or column\n\n    for i, png_file in enumerate(png_files):\n        img_path = os.path.join(folder_path, png_file)\n        img = mpimg.imread(img_path)\n        axes[i].imshow(img)\n        axes[i].set_title(png_file)\n        axes[i].axis('off')\n\n    # Hide any empty subplots\n    for j in range(i + 1, len(axes)):\n        axes[j].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_pngs_from_folder('/kaggle/working/')","metadata":{"_uuid":"05bce9cb-95c9-4b5c-9b51-8859335eed58","_cell_guid":"c1355b5e-180a-4848-a912-980b5f03b111","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''encoder = models[96][0]\ndecoder = models[96][1]\ncodebook = models[96][2]'''","metadata":{"_uuid":"1817b217-5f51-4f3c-bef7-9ae640064004","_cell_guid":"7a57ea55-7429-43ef-9e16-c80a6c91fed2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''for i in [90,84]:\n    models[i][0].save(f'epoch_{i}_encoder.keras')\n    models[i][1].save(f'epoch_{i}_decoder.keras')\n    np.save(f'epoch_{i}_codebook.npy', models[i][2].numpy())'''","metadata":{"_uuid":"519ff159-2a6f-40be-b346-883308b0fe60","_cell_guid":"d9f4a337-48e7-44a9-9708-746ee4c91b2d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_codebook_indices(imgs,codebook):\n    inputs = encoder(imgs)\n    embeddings=codebook\n    embedding_dim = tf.shape(codebook)[-1]\n    input_shape = tf.shape(inputs)\n    flat_inputs = tf.reshape(inputs, [-1, embedding_dim])\n    flat_inputs = tf.cast(flat_inputs, dtype=embeddings.dtype)\n    \n        # Compute distances to embedding vectors\n    '''distances = (\n            tf.reduce_sum(flat_inputs ** 2, axis=1, keepdims=True)\n            - 2 * tf.matmul(flat_inputs, self.embeddings, transpose_b=True)\n            + tf.reduce_sum(self.embeddings ** 2, axis=1)\n        )'''\n    flat_inputs_norm = tf.nn.l2_normalize(flat_inputs, axis=1)  # [N, D]\n    embeddings_norm = tf.nn.l2_normalize(embeddings, axis=1)  # [K, D]\n        \n        # Cosine similarity = dot product of normalized vectors\n    cosine_sim = tf.matmul(flat_inputs_norm, embeddings_norm, transpose_b=True)  # [N, K]\n        \n        # Convert similarity to distance (cosine distance = 1 - cosine similarity)\n    distances = 1.0 - cosine_sim\n    # Find nearest embedding index\n    encoding_indices = tf.argmin(distances, axis=1)\n    return tf.reshape(encoding_indices, input_shape[:-1])\n\ndef get_embeddings(indices, codebook):\n    # Flatten indices to 1D or 2D if needed\n    flat_indices = tf.reshape(indices, [-1])\n    \n    # Lookup embeddings\n    flat_embeddings = tf.nn.embedding_lookup(codebook, flat_indices)  # [B*H*W, D]\n\n    # Reshape back to match original spatial structure + D\n    out_shape = tf.concat([tf.shape(indices), [tf.shape(codebook)[-1]]], axis=0)\n    return tf.reshape(flat_embeddings, out_shape)","metadata":{"_uuid":"634eb42b-a5c1-4cca-b40c-b47e75ec954e","_cell_guid":"b40f03ae-128c-4ef0-863d-79f752b79272","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch = next(iter(vqvae_train))\n\n# Convert PerReplica to a real tensor — pick one replica (usually 0 is fine)\nsample_batch = strategy.experimental_local_results(batch)[5]\n\n# If it's (input, label) format\ntst = sample_batch[9].numpy()\n\n\nplt.imshow(tst)\nplt.show()\nimg = np.expand_dims(tst,0)\nout = decoder(get_embeddings(get_codebook_indices(img,codebook),codebook))\nprint(len(np.unique(get_codebook_indices(img,codebook))))\n#out = vqvae(img)\nplt.imshow(out[0])\nplt.show()","metadata":{"_uuid":"294eb463-b983-4446-823c-4b1f9dfe1d15","_cell_guid":"81595fd7-b202-42a0-b6a4-644ed5c34bb2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img = Image.open('/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg')\nimg = img.resize((128,128))\nimg = np.asarray(img)/255.0\nplt.imshow(img)\nplt.show()\nimg = np.expand_dims(img,0)\nout = decoder(get_embeddings(get_codebook_indices(img,codebook),codebook))\nplt.imshow(out[0])\nplt.show()","metadata":{"_uuid":"9358dd47-ad4d-47ef-9344-554799cc792b","_cell_guid":"6d3f4e79-4836-42bf-ac28-aa4d2470aeac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Total model losses:\", vqvae.losses)","metadata":{"_uuid":"4ef68d4a-2696-4a26-8ce5-a611a3ad0c34","_cell_guid":"162b3428-3967-4455-b6cb-3de6e53b5627","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 2048\nIMG_SIZE = (128, 128)\n\ndf = pd.read_csv('/kaggle/input/flickr30k/captions.txt')\nimage_paths = ['/kaggle/input/flickr30k/flickr30k_images/' + row.image_name for row in df.itertuples(index=False)]\ncaptions = [row.comment for row in df.itertuples(index=False)]\n\n# Save captions immediately\nnp.save('Captions.npy', captions)\n\n# ======================\n# tf.data Pipeline\n# ======================\ndef preprocess_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, IMG_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\ndef create_dataset(image_paths, batch_size):\n    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n    img_ds = path_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    img_ds = img_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return img_ds\n\n\nis_creating = int(input(\"Do you want to create the dataset ?\"))\nif is_creating:\n    dataset = create_dataset(image_paths, BATCH_SIZE)\n    \n    # ======================\n    # Tokenization Loop\n    # ======================\n    tokens_list = []\n    @tf.function\n    def process_batch(images):\n        return get_codebook_indices(images, codebook)\n        \n    with strategy.scope():\n        for batch_images in tqdm(dataset, total=len(image_paths) // BATCH_SIZE + 1):\n            tokens = process_batch(batch_images)\n            tokens_list.append(tokens)\n    \n        tokens_all = tf.concat(tokens_list, axis=0)  # Shape: (N, H, W)\n        tokens_seq = tf.reshape(tokens_all, [tokens_all.shape[0], -1])  # Shape: (N, 64)\n\n# ======================\n# Save Tokens\n# ======================\n    np.save('Tokens_seq.npy', tokens_seq.numpy())\n    print(\"✅ Tokenization complete and saved.\")\n\nelse:\n    captions = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_90/Captions.npy')\n    tokens_seq = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_90/Tokens_seq.npy').astype(np.int32)\n","metadata":{"_uuid":"ead0d3b9-f041-4790-af13-038c59360aea","_cell_guid":"3d2ad114-3c5b-41bc-b647-375ccfdd2301","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"longest_sentence = max(captions, key=lambda s: len(s.split()))\nword_count = len(longest_sentence.split())\nprint(longest_sentence)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\nimport re\n\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string) \n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 18500\nsequence_length = 85\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\n\nvectorizer.adapt(captions)\ntext_vect = tf.cast(vectorizer(captions),tf.int32)","metadata":{"_uuid":"7abfec3a-b164-49b2-a9a8-990d7fafa476","_cell_guid":"30a68179-ba3d-4088-aefd-dab55d4b84e3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(tokens_seq))\nprint(np.shape(text_vect))","metadata":{"_uuid":"81c3fbca-a405-4419-acb3-b3f0c9dc3fed","_cell_guid":"e72aff75-034d-4dbf-adee-842d1edcf6b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_tokens = tf.cast(tf.fill([len(text_vect), 1], num_embeddings),tf.int32)\ndecoder_tokens = tf.concat([start_tokens, tokens_seq], axis=1)","metadata":{"_uuid":"95c0a9dd-6b90-472e-9d4a-70063d4ecf9f","_cell_guid":"dd7d7171-6d1e-477f-afc3-8d96f694c599","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(decoder_tokens))","metadata":{"_uuid":"34887f96-bffa-4c8b-9998-93bb63db0c0f","_cell_guid":"23b6298a-1338-455f-a78d-4775fc43c22d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ex = tokens_seq[]\nprint(len(np.unique(ex)))","metadata":{"_uuid":"48e31b99-cac3-4045-8dfc-c930eddd0a70","_cell_guid":"37ea6df6-3639-4446-8f99-7bec4a07bb06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.min(tokens_seq)","metadata":{"_uuid":"ba89ced4-dccc-435a-a525-6851e92bd80d","_cell_guid":"958a22ee-6fa1-4206-9ece-324c17a2a022","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(np.unique(tokens_seq[9]))","metadata":{"_uuid":"7088bccd-d6e5-4230-908a-c68163647ad8","_cell_guid":"add5b155-a3c6-4e8b-8e1b-34dbc05107e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = tf.random.uniform(shape=(1, 16, 16), minval=100, maxval=254, dtype=tf.int32)\nflat_indices = tf.reshape(random_indices, [-1])\nquantized_vectors = tf.gather(codebook, flat_indices)\nquantized_vectors = tf.reshape(quantized_vectors, (1, 16, 16\n                                                   , embedding_dim))\n\n# Decode to image\ngenerated_image = decoder(quantized_vectors)\n# Display\nimport matplotlib.pyplot as plt\nplt.imshow(generated_image[0].numpy())\nplt.axis('off')\nplt.title('Random Image from VQ-VAE Codebook')\nplt.show()","metadata":{"_uuid":"448221fd-13a1-445e-b5bc-15925efb4591","_cell_guid":"77512927-b406-44b2-8dc7-ea511fdc2138","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}