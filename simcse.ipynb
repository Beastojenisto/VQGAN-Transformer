{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport numpy as np\nimport pandas as pd\nimport string \nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:45:25.635094Z","iopub.execute_input":"2025-08-23T11:45:25.635314Z","iopub.status.idle":"2025-08-23T11:45:37.840549Z","shell.execute_reply.started":"2025-08-23T11:45:25.635290Z","shell.execute_reply":"2025-08-23T11:45:37.839951Z"}},"outputs":[{"name":"stderr","text":"2025-08-23 11:45:26.806729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755949526.963767      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755949527.019817      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr8k/captions.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:51:17.195681Z","iopub.execute_input":"2025-08-23T11:51:17.195975Z","iopub.status.idle":"2025-08-23T11:51:17.249512Z","shell.execute_reply.started":"2025-08-23T11:51:17.195947Z","shell.execute_reply":"2025-08-23T11:51:17.248939Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df = df.groupby('image').head(2).reset_index(drop=True)\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:51:32.224113Z","iopub.execute_input":"2025-08-23T11:51:32.224795Z","iopub.status.idle":"2025-08-23T11:51:32.243638Z","shell.execute_reply.started":"2025-08-23T11:51:32.224760Z","shell.execute_reply":"2025-08-23T11:51:32.243094Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1001773457_577c3a7d70.jpg   \n3  1001773457_577c3a7d70.jpg   \n4  1002674143_1b742ab4b8.jpg   \n5  1002674143_1b742ab4b8.jpg   \n6  1003163366_44323f5815.jpg   \n7  1003163366_44323f5815.jpg   \n8  1007129816_e794419615.jpg   \n9  1007129816_e794419615.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2         A black dog and a spotted dog are fighting  \n3  A black dog and a tri-colored dog playing with...  \n4  A little girl covered in paint sits in front o...  \n5  A little girl is sitting in front of a large p...  \n6  A man lays on a bench while his dog sits by him .  \n7  A man lays on the bench to which a white dog i...  \n8     A man in an orange hat starring at something .  \n9            A man wears an orange hat and glasses .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1001773457_577c3a7d70.jpg</td>\n      <td>A black dog and a spotted dog are fighting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1001773457_577c3a7d70.jpg</td>\n      <td>A black dog and a tri-colored dog playing with...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1002674143_1b742ab4b8.jpg</td>\n      <td>A little girl covered in paint sits in front o...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1002674143_1b742ab4b8.jpg</td>\n      <td>A little girl is sitting in front of a large p...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1003163366_44323f5815.jpg</td>\n      <td>A man lays on a bench while his dog sits by him .</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1003163366_44323f5815.jpg</td>\n      <td>A man lays on the bench to which a white dog i...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1007129816_e794419615.jpg</td>\n      <td>A man in an orange hat starring at something .</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1007129816_e794419615.jpg</td>\n      <td>A man wears an orange hat and glasses .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df = df.drop(['image'],axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:51:38.717818Z","iopub.execute_input":"2025-08-23T11:51:38.718125Z","iopub.status.idle":"2025-08-23T11:51:38.722833Z","shell.execute_reply.started":"2025-08-23T11:51:38.718103Z","shell.execute_reply":"2025-08-23T11:51:38.722097Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def infonce_loss(zi,zj):\n    zi = tf.nn.l2_normalize(zi)\n    zj = tf.nn.l2_normalize(zj)\n    logits = tf.matmul(zi,(tf.transpose(zj)))\n    logits = tf.nn.softmax(logits)\n    labels = tf.eye(tf.shape(logits)[-1])\n    zi_zj_loss = tf.keras.losses.CategoricalCrossentropy(labels,logits)\n    zj_zi_loss = tf.keras.losses.CategoricalCrossentropy(labels,tf.transpose(logits))\n    total_loss = zi_zj_loss + zj_zi_loss\n    return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:51:45.276097Z","iopub.execute_input":"2025-08-23T11:51:45.276566Z","iopub.status.idle":"2025-08-23T11:51:45.281061Z","shell.execute_reply.started":"2025-08-23T11:51:45.276542Z","shell.execute_reply":"2025-08-23T11:51:45.280355Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"16182"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, mask_zero = True, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim,mask_zero=mask_zero)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim,mask_zero=False)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n\n        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-5)\n        self.dropout_1 = layers.Dropout(0.1)\n\n    def call(self, inputs, mask=None):\n        # Convert mask to boolean with shape (batch, 1, seq_len)\n        if mask is not None:\n            mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.bool)\n\n        attention_output = self.attention(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=mask\n        )\n        attention_output = self.dropout_1(attention_output)\n        proj_input = self.layernorm_1(inputs + attention_output)\n\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n\n        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.dropout_1 = layers.Dropout(0.1)\n        self.dropout_2 = layers.Dropout(0.1)\n        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_3 = layers.LayerNormalization(epsilon=1e-5)\n\n    # def get_causal_attention_mask(self, inputs):\n    #     seq_len = tf.shape(inputs)[1]\n    #     causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=tf.bool), -1, 0)\n    #     return causal_mask[tf.newaxis, :, :]  # (1, seq_len, seq_len)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        # Padding mask: (batch_size, 1, seq_len)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.bool)\n        else:\n            padding_mask = None\n\n        # # Causal mask: (1, seq_len, seq_len)\n        # causal_mask = self.get_causal_attention_mask(inputs)\n\n        # # Combine masks for self-attention\n        # if padding_mask is not None:\n        #     combined_mask = tf.logical_and(padding_mask, causal_mask)\n        # else:\n        #     combined_mask = causal_mask\n\n        # Self-attention with combined mask\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=None,\n            use_causal_mask=True\n        )\n        attention_output_1 = self.dropout_1(attention_output_1)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n\n        # Cross-attention with padding mask only\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n            use_causal_mask=False\n        )\n        attention_output_2 = self.dropout_2(attention_output_2)\n        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    embed_dim = 256\n    dense_dim = 1024\n    num_heads = 8\n    num_blocks = 10\n    \n    encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n    decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n    \n    # Padding masks\n    encoder_mask = tf.keras.layers.Lambda(lambda x: tf.cast(tf.not_equal(x, 0), tf.bool))(encoder_inputs)\n    # cross_attention_mask = tf.keras.layers.Lambda(lambda x: tf.cast(x[:, tf.newaxis, tf.newaxis, :], tf.bool))(encoder_mask) \n    \n    # Embeddings\n    encoder_embed = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n    decoder_embed = PositionalEmbedding(256, 257, embed_dim,mask_zero=False)(decoder_inputs)\n    \n    # Encoder blocks\n    x = encoder_embed\n    for _ in range(num_blocks):\n        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x, mask=encoder_mask)\n    encoder_outputs = x\n    \n    # Decoder blocks\n    x = decoder_embed\n    for _ in range(num_blocks):\n        x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs, mask=encoder_mask)\n    \n    # Final layers\n    x = layers.LayerNormalization(epsilon=1e-5)(x)\n    x = layers.Dropout(0.1)(x)\n    decoder_outputs = layers.Dense(256)(x)\n\n    transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(xi, xj):\n    with tf.GradientTape() as tape:\n        zi = transformer(xi, training=True)\n        zj = transformer(xj, training=True)\n\n        loss = infonce_loss(zi,zj)\n        gradients = tape.gradient(loss, transformer.trainable_variables)\n        optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))    \n        return loss    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}