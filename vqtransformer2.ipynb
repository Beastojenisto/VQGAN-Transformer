{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12919138,"sourceType":"datasetVersion","datasetId":8013533}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom IPython.display import clear_output\nimport numpy as np","metadata":{"_uuid":"e4228008-a588-4180-9680-ff34692c860c","_cell_guid":"e732a76f-7c01-4b6b-8d9a-f96f0b361732","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver('local')\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    print('die')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_embeddings=256\nembedding_dim=256","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import register_keras_serializable\nfrom tensorflow.keras.layers import Conv2D, Add\n\n@register_keras_serializable(package=\"Custom\")\nclass SelfAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.filters = input_shape[-1]\n        self.f = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.g = Conv2D(self.filters // 8, kernel_size=1, padding='same')\n        self.h = Conv2D(self.filters, kernel_size=1, padding='same')\n        super().build(input_shape)\n\n    def call(self, x):\n        f = self.f(x)  # (B, H, W, C//8)\n        g = self.g(x)\n        h = self.h(x)  # (B, H, W, C)\n\n        shape_f = tf.shape(f)\n        B, H, W = shape_f[0], shape_f[1], shape_f[2]\n\n        f_flat = tf.reshape(f, [B, H * W, self.filters // 8])\n        g_flat = tf.reshape(g, [B, H * W, self.filters // 8])\n        h_flat = tf.reshape(h, [B, H * W, self.filters])\n\n        beta = tf.nn.softmax(tf.matmul(f_flat, g_flat, transpose_b=True), axis=-1)  # (B, N, N)\n\n        o = tf.matmul(beta, h_flat)  # (B, N, C)\n        o = tf.reshape(o, [B, H, W, self.filters])\n\n        return Add()([x, o])  # Residual connection\n\n    def get_config(self):\n        config = super().get_config()\n        # If you have custom arguments in __init__, add them here\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"captions = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_78/Captions.npy')\ntokens_seq = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_78/Tokens_seq.npy').astype(np.int32)\nencoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts/vqgan_epoch_78/epoch_78_encoder.keras',custom_objects={'SelfAttention': SelfAttention})\ndecoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts/vqgan_epoch_78/epoch_78_decoder.keras',custom_objects={'SelfAttention': SelfAttention})\ncodebook = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_78/epoch_78_codebook.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\nimport re\n\nstrip_chars = string.punctuation + \"Â¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvocab_size = 18500\nsequence_length = 85\n\nvectorizer = layers.TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\n\nvectorizer.adapt(captions)\ntext_vect = tf.cast(vectorizer(captions),tf.int32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_tokens = tf.cast(tf.fill([len(text_vect), 1], 256),tf.int32)\ndecoder_tokens = tf.concat([start_tokens, tokens_seq], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = tf.random.uniform(shape=(1, 16, 16), minval=100, maxval=254, dtype=tf.int32)\nflat_indices = tf.reshape(random_indices, [-1])\nquantized_vectors = tf.gather(codebook, flat_indices)\nquantized_vectors = tf.reshape(quantized_vectors, (1, 16, 16\n                                                   , embedding_dim))\n\n# Decode to image\ngenerated_image = decoder(quantized_vectors)\n# Display\nimport matplotlib.pyplot as plt\nplt.imshow(generated_image[0].numpy())\nplt.axis('off')\nplt.title('Random Image from VQ-VAE Codebook')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer_dataset = (\n    tf.data.Dataset.from_tensor_slices((\n        (tf.convert_to_tensor(text_vect, dtype=tf.int32),\n         tf.convert_to_tensor(decoder_tokens[:, :-1], dtype=tf.int32)),\n        tf.convert_to_tensor(decoder_tokens[:, 1:], dtype=tf.int32)\n    ))\n    .batch(512)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\ntransformer_dataset = strategy.experimental_distribute_dataset(transformer_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n\n        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-5)\n        self.dropout_1 = layers.Dropout(0.1)\n\n    def call(self, inputs, mask=None):\n        # Convert mask to boolean with shape (batch, 1, seq_len)\n        if mask is not None:\n            mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.bool)\n\n        attention_output = self.attention(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=mask\n        )\n        attention_output = self.dropout_1(attention_output)\n        proj_input = self.layernorm_1(inputs + attention_output)\n\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)","metadata":{"_uuid":"e0631f6d-213f-45c4-b2e5-ef2e6b700700","_cell_guid":"36616b65-5de4-4a45-86ab-ddd9262be59b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n\n        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(embed_dim/num_heads))\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.dropout_1 = layers.Dropout(0.1)\n        self.dropout_2 = layers.Dropout(0.1)\n        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-5)\n        self.layernorm_3 = layers.LayerNormalization(epsilon=1e-5)\n\n    # def get_causal_attention_mask(self, inputs):\n    #     seq_len = tf.shape(inputs)[1]\n    #     causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=tf.bool), -1, 0)\n    #     return causal_mask[tf.newaxis, :, :]  # (1, seq_len, seq_len)\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        # Padding mask: (batch_size, 1, seq_len)\n        # if mask is not None:\n        #     padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.bool)\n        # else:\n        #     padding_mask = None\n\n        # # Causal mask: (1, seq_len, seq_len)\n        # causal_mask = self.get_causal_attention_mask(inputs)\n\n        # # Combine masks for self-attention\n        # if padding_mask is not None:\n        #     combined_mask = tf.logical_and(padding_mask, causal_mask)\n        # else:\n        #     combined_mask = causal_mask\n\n        # Self-attention with combined mask\n        attention_output_1 = self.attention_1(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=None,\n            use_causal_mask=True\n        )\n        attention_output_1 = self.dropout_1(attention_output_1)\n        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n\n        # Cross-attention with padding mask only\n        attention_output_2 = self.attention_2(\n            query=attention_output_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=mask,\n            use_causal_mask=False\n        )\n        attention_output_2 = self.dropout_2(attention_output_2)\n        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n\n        proj_output = self.dense_proj(attention_output_2)\n        return self.layernorm_3(attention_output_2 + proj_output)","metadata":{"_uuid":"b811abc7-34a0-4e25-9381-fbb651afc51c","_cell_guid":"1ad6322c-781d-493b-86e2-f7b9a0203261","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, mask_zero = True, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=input_dim, output_dim=output_dim,mask_zero=mask_zero)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim,mask_zero=False)\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions","metadata":{"_uuid":"70f6a537-81ae-4a36-bd49-761dd47067f5","_cell_guid":"7f02f91c-72ba-43e6-b5ea-213e4218f6fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    embed_dim = 256\n    dense_dim = 1536\n    num_heads = 8\n    num_blocks = 6\n    \n    encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n    decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n    \n    # Padding masks\n    encoder_mask = tf.keras.layers.Lambda(lambda x: tf.cast(tf.not_equal(x, 0), tf.bool))(encoder_inputs)\n    cross_attention_mask = tf.keras.layers.Lambda(lambda x: tf.cast(x[:, tf.newaxis, tf.newaxis, :], tf.bool))(encoder_mask) \n    \n    # Embeddings\n    encoder_embed = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n    decoder_embed = PositionalEmbedding(256, 257, embed_dim,mask_zero=False)(decoder_inputs)\n    \n    # Encoder blocks\n    x = encoder_embed\n    for _ in range(num_blocks):\n        x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x, mask=encoder_mask)\n    encoder_outputs = x\n    \n    # Decoder blocks\n    x = decoder_embed\n    for _ in range(num_blocks):\n        x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs, mask=cross_attention_mask)\n    \n    # Final layers\n    x = layers.LayerNormalization(epsilon=1e-5)(x)\n    x = layers.Dropout(0.1)(x)\n    decoder_outputs = layers.Dense(256)(x)\n\n    transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"_uuid":"f3495af2-06b9-4c1b-83e0-12831fb994f6","_cell_guid":"15825ccd-df0d-429f-b8ff-35a467833b5a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformer.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_token = 256\nmax_output_length = 256\n\ndef generate_image_tokens(input_text):\n    # Vectorize input text\n    tokenized_text = vectorizer([input_text])  # Shape: (1, text_seq_len)\n\n    # Start the decoded sequence with the start token\n    decoded_image_tokens = [start_token]\n\n    for i in range(max_output_length):\n        # Convert to proper input format\n        decoder_input = tf.convert_to_tensor([decoded_image_tokens])\n\n        # Predict next token probabilities\n        predictions = (transformer([tokenized_text, decoder_input]))\n        \n\n        # Get the token for the current step\n        sampled_token_index = np.argmax(predictions[0, -1, :])\n\n        # Append token to sequence\n        decoded_image_tokens.append(sampled_token_index)\n\n    # Optionally decode tokens into an image here\n    return decoded_image_tokens\n\ndef get_embeddings(indices, codebook):\n    flat_indices = tf.reshape(indices, [-1])\n    flat_embeddings = tf.nn.embedding_lookup(codebook, flat_indices)\n\n    out_shape = tf.concat([tf.shape(indices), [tf.shape(codebook)[-1]]], axis=0)\n    return tf.reshape(flat_embeddings, (16,16,256))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageGeneration(tf.keras.callbacks.Callback):\n    def __init__(self, txt):\n        super().__init__()\n        self.txt = txt\n    def on_epoch_end(self,epoch):\n        if epoch % 10 == 0:\n            clear_output(wait=True)\n            print(f\"Epoch:{epoch}\")\n            print(f\"Prompt:{self.txt}\")\n            tokens = generate_image_tokens(self.txt)\n            image = get_embeddings(output_tokens,codebook)\n            plt.imshow(image[0])\n            plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    callback = ImageGeneration(\"A child is falling off a slide onto colored balloons floating on a pool of water\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    def perplexity(y_true, y_pred):\n        loss = loss_fn(y_true, y_pred)\n        return tf.exp(tf.reduce_mean(loss))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    transformer.compile(loss=loss_fn,\n                       optimizer=optimizer,\n                       metrics=['accuracy',perplexity])\n    transformer.fit(transformer_dataset,epochs=100, callbacks=[callback])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_tokens = generate_image_tokens(\"A man wearing a red shirt in walking in the road\")[1:]\nprint(np.shape(output_tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decoder = tf.keras.models.load_model('/kaggle/input/vqgan-parts/vqgan_epoch_90/epoch_90_decoder.keras',custom_objects={'SelfAttention': SelfAttention})\ncodebook = np.load('/kaggle/input/vqgan-parts/vqgan_epoch_90/epoch_90_codebook.npy')\nimg = decoder(np.expand_dims(get_embeddings(output_tokens,codebook),0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.shape(get_embeddings(output_tokens,codebook)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(img[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(img[0])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}